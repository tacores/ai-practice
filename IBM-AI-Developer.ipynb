{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1013bc1-3f99-4f2f-b653-6cdc3bb8ff7a",
   "metadata": {},
   "source": [
    "# IBM AI開発者"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c13632-7634-4422-9b70-a87686e4ebc7",
   "metadata": {},
   "source": [
    "## モデルの種類"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f2f176-e88c-49ee-a80d-52d95556fee1",
   "metadata": {},
   "source": [
    "BLIP（Bootstrapped Language-Image Pre-training）視覚と言語の統合タスクに対応\n",
    "LLM (Large Language Models) 大規模言語モデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee0091b-6293-4ea4-a90e-1c3ddccfd144",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "596fd99c-19d7-4bf8-81ce-7f70a7c6fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a man and woman riding on a bicycle\n"
     ]
    }
   ],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "# Initialize the processor and model from Hugging Face\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# Load an image\n",
    "image = Image.open(\"images/cycling.jpg\")\n",
    "# Prepare the image\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "# Generate captions\n",
    "outputs = model.generate(**inputs)\n",
    "caption = processor.decode(outputs[0],skip_special_tokens=True)\n",
    " \n",
    "print(\"Generated Caption:\", caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bb705d-6869-42ed-9b82-d1c660ea26a1",
   "metadata": {},
   "source": [
    "### 条件付きラベル生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf6661a-2af9-4abf-91ed-925d4dcdaac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a photography of a couple riding a bike\n",
      "a man and woman riding on a bicycle\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "# Load an image\n",
    "image = Image.open(\"images/cycling.jpg\")\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(image, text, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "# unconditional image captioning\n",
    "inputs = processor(image, return_tensors=\"pt\")\n",
    "out = model.generate(**inputs)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb0594-abb8-48a6-9f1e-4c4b2e5a7eb1",
   "metadata": {},
   "source": [
    "## 画像分類（UI付き）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56ff5c0-731f-490e-a549-a09ce12bf671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画像分類モデル\n",
    "import torch\n",
    "# PyTorch Hubから\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True).eval()\n",
    "\n",
    "# 予測関数\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "# Download human-readable labels for ImageNet.\n",
    "response = requests.get(\"https://git.io/JJkYN\")\n",
    "labels = response.text.split(\"\\n\")\n",
    "\n",
    "def predict(inp):\n",
    "    inp = transforms.ToTensor()(inp).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.nn.functional.softmax(model(inp)[0], dim=0)\n",
    "        confidences = {labels[i]: float(prediction[i]) for i in range(1000)}\n",
    "    return confidences\n",
    "\n",
    "# Gradioインターフェース\n",
    "import gradio as gr\n",
    "gr.Interface(fn=predict,\n",
    "       inputs=gr.Image(type=\"pil\"),\n",
    "       outputs=gr.Label(num_top_classes=3),\n",
    "       examples=[\"/content/lion.jpg\", \"/content/cheetah.jpg\"]).launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f4b4f6-8499-4a17-81a0-71c458660521",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa2d57-7b21-43d0-9e1d-c1214b1481ed",
   "metadata": {},
   "source": [
    "### チャットボット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3765eb89-34d6-4cb0-a1e3-8237849447e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "# Load model (download on first run and reference local installation for consequent runs)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "conversation_history = []\n",
    "while True:\n",
    "    # Create conversation history string\n",
    "    history_string = \"\\n\".join(conversation_history)\n",
    "    # Get the input data from the user\n",
    "    input_text = input(\"> \")\n",
    "    # Tokenize the input text and history\n",
    "    inputs = tokenizer.encode_plus(history_string, input_text, return_tensors=\"pt\")\n",
    "    # Generate the response from the model\n",
    "    outputs = model.generate(**inputs)\n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    \n",
    "    print(response)\n",
    "    # Add interaction to conversation history\n",
    "    conversation_history.append(input_text)\n",
    "    conversation_history.append(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a175465-a024-42c9-89bc-11f109464537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"facebook/blenderbot-400M-distill\"\n",
    "\n",
    "# Load model (download on first run and reference local installation for consequent runs)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3115e33-5ac1-4b5d-bbf1-8dedd399c9de",
   "metadata": {},
   "source": [
    "### Meta Llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b251fb-5002-49ea-9bf0-71ccabaa6846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "pipeline(\"Hey how are you doing today?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552fea6c-fc60-4c60-b8e2-5e006c859022",
   "metadata": {},
   "source": [
    "### LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5599c0-fef2-4f27-ab11-92102478f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# OpenAIの言語モデルをロード\n",
    "llm = OpenAI(api_key=\"your-openai-api-key\")\n",
    "\n",
    "# プロンプトテンプレートを定義\n",
    "template = PromptTemplate(template=\"Translate the following English text to French: {text}\")\n",
    "\n",
    "# チェーンを作成\n",
    "chain = LLMChain(llm=llm, prompt_template=template)\n",
    "\n",
    "# 入力を与えて出力を取得\n",
    "response = chain.run(text=\"Hello, how can you assist me today?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ca3633-e3a3-419e-ae68-c3dd3bbad146",
   "metadata": {},
   "source": [
    "## 音声"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9052a1a-39ff-4e14-a253-9a49d1cb2e93",
   "metadata": {},
   "source": [
    "### OpenAI Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6832fd2-9555-4cfe-acd9-c683f4abdb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/openai/whisper.git \n",
    "\n",
    "# sudo apt update\n",
    "# sudo apt install ffmpeg -y\n",
    "# も必要\n",
    "\n",
    "# https://github.com/openai/whisper 参照"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fa066c-39dd-40d4-8838-454f2e63238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# オーディオファイルから\n",
    "import whisper\n",
    "\n",
    "# Load the Whisper model\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# Transcribe the audio file\n",
    "result = model.transcribe(\"audio_example.mp3\")\n",
    "\n",
    "# Output the transcription\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ee728-f21e-43da-a09c-e421d02e0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# リアルタイム\n",
    "from flask import Flask, request\n",
    "import whisper\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "@app.route(\"/transcribe\", methods=[\"POST\"])\n",
    "def transcribe_audio():\n",
    "    audio_file = request.files[\"audio\"]\n",
    "    result = model.transcribe(audio_file)\n",
    "    return {\"transcription\": result[\"text\"]}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575a9d45-d42a-4f64-90c7-37b29ebc9985",
   "metadata": {},
   "source": [
    "## ツール類"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf7a54f-91b7-4761-a93e-a2f01524ec23",
   "metadata": {},
   "source": [
    "### Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24696c6-644d-4d63-840f-819f997a792f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gradio\n",
    "import gradio as gr\n",
    "def greet(name, intensity):\n",
    "  return \"Hello, \" + name + \"!\" * int(intensity)\n",
    "demo = gr.Interface(\n",
    "  fn=greet,\n",
    "  inputs=[\"text\", \"slider\"],\n",
    "  outputs=[\"text\"],\n",
    ")\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b000ff76-52e3-4d71-897b-ea45631fc59d",
   "metadata": {},
   "source": [
    "### Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef259d5-7411-4a9b-bbb0-f6573563a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
